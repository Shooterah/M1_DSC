{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907d3420-df16-478d-b84b-ef4d058b2f47",
   "metadata": {},
   "source": [
    "# Do you want to build a deep learning framework?\n",
    "\n",
    "Come on let's go and build it.\n",
    "\n",
    "We'll build the *pancake* deep learning framework, a toy but complete deep learning framework similar to pytorch.\n",
    "The goal is to illustrate the main principles and how the basics of \"automatic differentiation\" work.\n",
    "\n",
    "It doesn't have to be a full framework.\n",
    "\n",
    "For the sake of time, we will limit the implementation to a composition of functions, with trainable parameters, with no complex graph structure (so something like pytorch's `nn.Sequential`) but we could handle any computational graph with more work (like having a custom tensor class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a59136-6d92-449c-b07e-ec83412df06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "SEP = \"\\n======================\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4b298-b855-4865-82b3-dc319c7c369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* *********************************************************** */\n",
    "/* styling the notebook, you can ignore it if it does not work */\n",
    "/* *********************************************************** */\n",
    "h3 { color: #60a5fa !important; text-decoration: underline; font-variant-caps: small-caps;}\n",
    ".jp-OutputArea-output { border-left: 10px solid grey; margin-left: 20px; }\n",
    ".spoiler { background: black; color: black;  margin-bottom: .1em; }\n",
    ".spoiler:hover { background: white; transition: background 1s 1s; }\n",
    ".bigmath { font-size: 120%; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44c756-e6df-4906-8801-5b480fe7b0cf",
   "metadata": {},
   "source": [
    "### The chain rule\n",
    "\n",
    "<div class=\"bigmath\">\n",
    "\n",
    "Let's start by recalling the principle of the chain rule, then we will be able to implement a framework using it.\n",
    "\n",
    "Imagine we compute $E = (k \\cdot v)^2$ with $v$ the \"input\" and $E$ the \"output\", and $k$ a constant parameter.\n",
    "\n",
    "We can write the computation as a composition of functions:\n",
    "- $E = F(v) = f(g_k(v))$ with\n",
    "    - $f: x \\mapsto x^2$.\n",
    "    - $g_k: x \\mapsto k \\cdot x$\n",
    "    \n",
    "The chain rule tell us that the derivative at a given value $v_0$ is $F'(v_0) = f'(g_k(v_0)) g_k'(v_0)$, or using a different notation:\n",
    "- naming $M = g_k(v)$ and thus $E = f(M)$\n",
    "- $\\frac{\\partial E}{\\partial v}(v_0)$, which tells us how a small modification of $v$ (around $v_0$) will increase $E = F(v) = f(M)$, which is the information used by gradient descent,\n",
    "- is equal to $\\frac{\\partial E}{\\partial M}(g_k(v_0)) \\frac{\\partial M}{\\partial v}(v_0)$\n",
    "- the notational shortcut $\\frac{\\partial E}{\\partial M} \\frac{\\partial M}{\\partial v}$ is often used and all this generalizes well to a composition of more than two fonctions.\n",
    "    \n",
    "Let's program this example, noting that we need $g_k(v_0)$ to compute the gradient, that is $\\frac{\\partial E}{\\partial v}(v_0)$.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b91667-9b92-483c-870b-5ae2810eb907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting our \"constant\" (which could become our trainable parameter later)\n",
    "k = 0.5\n",
    "\n",
    "# choosing an input to our function (v0 above)\n",
    "v = 42    # v0\n",
    "\n",
    "# computing the intermediate values, using the functions (g and f)\n",
    "M = k * v # g(v0)\n",
    "E = M**2  # f(g(v0))\n",
    "print(\"E is\", E)\n",
    "\n",
    "# computing the derivative using the chain rule\n",
    "dE_dM = 2*M  # derivating M² wrt M gives 2M\n",
    "dM_dv = k    # derivation k.v wrt to v gives k\n",
    "dE_dv = dE_dM * dM_dv\n",
    "print(\"The gradient, computed with the chain rule, of E with respect to v is\", dE_dv)\n",
    "\n",
    "ε = 0.001\n",
    "v2 = v + ε\n",
    "M2 = k * v2\n",
    "E2 = M2**2\n",
    "check_slope = (E2 - E) / ε\n",
    "print(\"The numerical approximation of the slope (gradient) is\", check_slope)\n",
    "\n",
    "# similarly\n",
    "dM_dk = v  # derivating k.v wrt to k gives v\n",
    "dE_dk = dE_dM * dM_dk # we can reuse dE_dM that we already computed\n",
    "print(\"Changing parameter k would increase E\", dE_dk, \"times more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8896f8-7215-45b1-909e-daf2c309813a",
   "metadata": {},
   "source": [
    "<div class=\"bigmath\">\n",
    "\n",
    "Let's remind that we could have a composition of more functions $F(x) = f_4(f_3(f_2(f_1(x))))$.\n",
    "\n",
    "The chain rule becomes, by applying it several times (and reversing the writing order):\n",
    "\n",
    "$$F'(x) = f'_1(x) \\cdot f'_2(f_1(x)) \\cdot f'_3(f_2(f_1(x))) \\cdot f'_4(f_3(f_2(f_1(x))))$$\n",
    "    \n",
    "If we call $F1 = f1(x)$, $F2 = f2(F1)$ etc, then we can write the chain rule (and the gradient) as\n",
    "    \n",
    "$$\\frac{\\partial F4}{\\partial x} = \\frac{\\partial F4}{\\partial F3} \\frac{\\partial F3}{\\partial F2} \\frac{\\partial F2}{\\partial F1} \\frac{\\partial F1}{\\partial x}$$\n",
    "or with aggregation\n",
    "$$\\frac{\\partial F4}{\\partial x} = \\frac{\\partial F4}{\\partial F2} \\frac{\\partial F2}{\\partial F1} \\frac{\\partial F1}{\\partial x}$$\n",
    "or with more aggregation\n",
    "$$\\frac{\\partial F4}{\\partial x} = \\frac{\\partial F4}{\\partial F1} \\frac{\\partial F1}{\\partial x}$$\n",
    "\n",
    "    \n",
    "or in the reverse order to make it more clear that it is similar to the other notation\n",
    "\n",
    "$$\\frac{\\partial F4}{\\partial x} = \\frac{\\partial F1}{\\partial x} \\frac{\\partial F2}{\\partial F1} \\frac{\\partial F3}{\\partial F2} \\frac{\\partial F4}{\\partial F3} $$\n",
    "or with aggregation\n",
    "$$\\frac{\\partial F4}{\\partial x} = \\frac{\\partial F1}{\\partial x} \\frac{\\partial F2}{\\partial F1} \\frac{\\partial F4}{\\partial F2} $$\n",
    "or with more aggregation\n",
    "$$\\frac{\\partial F4}{\\partial x} = \\frac{\\partial F1}{\\partial x} \\frac{\\partial F4}{\\partial F1} $$\n",
    "\n",
    "Imagining $f1$ has some parameters $w1$ (like the constant $k$ above, but as a learnable parameter), we can also compute the gradient with respect to $w1$ using the chain rule, more precisely with (which reuses terms that we already have above)\n",
    "    \n",
    "$$\\frac{\\partial F4}{\\partial w1} = \\frac{\\partial F1}{\\partial w1} \\frac{\\partial F2}{\\partial F1} \\frac{\\partial F3}{\\partial F2} \\frac{\\partial F4}{\\partial F3} = \\frac{\\partial F1}{\\partial w1} \\frac{\\partial F4}{\\partial F1} $$\n",
    "\n",
    "... and similarly for $w2$\n",
    "    \n",
    "$$\\frac{\\partial F4}{\\partial w2} = \\frac{\\partial F2}{\\partial w2} \\frac{\\partial F3}{\\partial F2} \\frac{\\partial F4}{\\partial F3} = \\frac{\\partial F2}{\\partial w2} \\frac{\\partial F4}{\\partial F2} $$\n",
    "\n",
    "... and similarly for $w3$\n",
    "\n",
    "$$\\frac{\\partial F4}{\\partial w3} = \\frac{\\partial F3}{\\partial w3} \\frac{\\partial F4}{\\partial F3} $$\n",
    "\n",
    "... and $\\frac{\\partial F4}{\\partial w4}$ is \"simply\" the derivative of the know function $f_4$ with respect to its parameters.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc3296-c148-43c5-8894-ed9d7836c8b9",
   "metadata": {},
   "source": [
    "### Let's make *pancakes*\n",
    "\n",
    "We will compose functions that take some inputs, may have (trainable) parameters, and for which we need both the function (*forward* computation) itself and its derivative (the *backward* computation, that ).\n",
    "We will package all this in a abstract class called `Function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671ed48-3c8c-488f-afef-e1dab9658c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "    def compute_forward(self, x): raise NotImplementedError\n",
    "    def compute_backward(self, dObj_dOut): raise NotImplementedError\n",
    "    def get_params_and_grads(self): return [], []\n",
    "    def __call__(self, *args, **kwargs): return self.compute_forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb9260d-ad2b-4d8c-b67d-32a98ce5bf14",
   "metadata": {},
   "source": [
    "Important points:\n",
    "- we will suppose that $x$ has a first dimension that is the \"minibatch\" dimension (the same processing should be applied to all points in the minibatch).\n",
    "- the backward computation receives the gradient of the objective function wrt to the output of the function and it should apply the chain rule to compute the gradient of the same objective wrt to its input, i.e. dObj_dIn = dObj_dOut * dOut_dIn  (it should implement d_In).\n",
    "- if the function has trainable parameters\n",
    "    - it should return references to the parameters and a storage area for their gradient (e.g., `w, grad_w` instead of empty lists `[], []`)\n",
    "    - it should also accumulate, in the backward, the gradient wrt to parameters, e.g. `grad_w += dObj_dOut * dOut_dw`)\n",
    "    \n",
    "To understand the principle, let's do a very simple \"square\" function $x \\mapsto x^2$ and a function that multiplies by a trainable parameter $x \\mapsto k x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa82dda-6109-4a55-80de-caf7d466dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square(Function):\n",
    "    def compute_forward(self, x):\n",
    "        # remember the input as we need it to compute the gradient\n",
    "        self.input = x\n",
    "        return x**2 # the actual formula\n",
    "    \n",
    "    def compute_backward(self, dObj_dOut):\n",
    "        return 2*self.input * dObj_dOut # the formula for dOut_dIn is 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29125387-286a-42dd-8bb9-861e134d4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(Function):\n",
    "    def __init__(self):\n",
    "        self.k = np.random.uniform(0.499, 0.501, (1,)) # must be an array\n",
    "        self.grad_k = np.zeros(self.k.shape)\n",
    "    def get_params_and_grads(self):\n",
    "        # there is a single parameter but in general there might be more, so it is a list\n",
    "        return [self.k], [self.grad_k]\n",
    "    def compute_forward(self, x):\n",
    "        self.input = x\n",
    "        return self.k * x\n",
    "    def compute_backward(self, dObj_dOut):\n",
    "        self.grad_k += self.input * dObj_dOut\n",
    "        return self.k * dObj_dOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770ceff-6301-42f7-90aa-fdb13256b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually compose functions (following our initial example)\n",
    "g = Mul()\n",
    "f = Square()\n",
    "v = 42\n",
    "M = g(v)\n",
    "E = f(M)\n",
    "print(\"E is\", E)\n",
    "dE_dE = 1\n",
    "dE_dM = f.compute_backward(dE_dE)\n",
    "print(dE_dM)\n",
    "dE_dv = g.compute_backward(dE_dM) # this also updates grad_k\n",
    "print(dE_dv)\n",
    "\n",
    "print(\"The gradient wrt to the parameter k is\", g.grad_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc212984-03ce-4519-b513-12ac43752323",
   "metadata": {},
   "source": [
    "### Let's automate the composition\n",
    "\n",
    "We will create a meta function that applies a list of function one after the other, and does the reverse for the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc895034-3ee9-4799-8f92-7e5db4f5b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Function):\n",
    "    def __init__(self, functions):\n",
    "        self.functions = functions\n",
    "        # join the parameters of all functions\n",
    "        self.params = sum((f.get_params_and_grads()[0] for f in self.functions), [])\n",
    "        self.grad_params = sum((f.get_params_and_grads()[1] for f in self.functions), [])\n",
    "    def get_params_and_grads(self):\n",
    "        return self.params, self.grad_params\n",
    "    def compute_forward(self, x):\n",
    "        for f in self.functions:\n",
    "            x = f(x)\n",
    "        return x\n",
    "    def compute_backward(self, dObj):\n",
    "        for f in reversed(self.functions):\n",
    "            dObj = f.compute_backward(dObj)\n",
    "        return dObj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3af6ca-6804-48f1-91f3-bdb5109bbf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = Sequential([Mul(), Square()])\n",
    "v = 42\n",
    "E = composite(v)\n",
    "print(\"E is\", E)\n",
    "print(\"The uncomputed gradient wrt to all parameters is\", composite.get_params_and_grads()[1])\n",
    "composite.compute_backward(dE_dE)\n",
    "print(\"The gradient wrt to all parameters is\", composite.get_params_and_grads()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906df331-460f-4cab-ae69-eb6ddc3e5523",
   "metadata": {},
   "source": [
    "### Let's deep learn\n",
    "\n",
    "We will create a Linear layer and a sigmoid activation function so we can do a perceptron and run gradient descent on its parameters.\n",
    "We give the full sigmoid code.\n",
    "\n",
    "We also give the code for a function that computes the loss (given a prediction and the target value from the dataset).\n",
    "\n",
    "The **challenge** is to implement the forward and backward of the linear layer, if/when it works, the train loop below should run and find a good solution.\n",
    "You will probably have to iterate over your solution to fix possible errors you've made in your initial tries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cec0b7-e4b2-4195-87bf-a9b191da9727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Function):\n",
    "    def compute_forward(self, x):\n",
    "        # we return the element-wise sigmoid value, and store it as the derivative is easily expressed using it\n",
    "        self.output = 1 / (1 + np.exp(-x))\n",
    "        return self.output\n",
    "    def compute_backward(self, dObj_dOut):\n",
    "        return self.output * (1 - self.output) * dObj_dOut\n",
    "\n",
    "class BinaryCrossEntropy(Function): # negative log-likelihood\n",
    "    def compute_forward(self, pred, target):\n",
    "        self.input = pred\n",
    "        self.target = target\n",
    "        # np.where is a kind of \"if\" in numpy\n",
    "        return - np.log(np.where(self.target==1, pred, 1-pred))\n",
    "    def compute_backward(self):\n",
    "        return -1 / np.where(self.target==1, self.input, -(1-self.input))\n",
    "\n",
    "class Linear(Function):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        # I = n_in = number of input neurons\n",
    "        # O = n_out = number of output neurons\n",
    "        self.w = np.random.normal(0, 1/n_in, (n_out, n_in)) # shape (O, I)\n",
    "        self.b = np.random.normal(0, 0.1, (n_out,)) # shape (O)\n",
    "        self.grad_w = np.zeros(self.w.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "    def get_params_and_grads(self):\n",
    "        return [self.w, self.b], [self.grad_w, self.grad_b]\n",
    "    def compute_forward(self, x):\n",
    "        # B = minibatch size\n",
    "        self.input = x # shape (B, I)\n",
    "        # TODO for you: compute the output of size (B, O) using reshaping, broadcasting and a sum \n",
    "        return self.b + np.sum(..., axis=...)\n",
    "    def compute_backward(self, dObj_dOut):\n",
    "        # dObj_dOut is of shape (B, O)\n",
    "        # compute and accumulate the gradient of the objective with respect to the parameters\n",
    "        # np.sum over the minibatch elements\n",
    "        self.grad_w += ...\n",
    "        self.grad_b += ...\n",
    "        # return the gradient of the objective with respect to the input\n",
    "        return np.sum(..., axis=...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a11539e-502f-4574-b37d-3217ed25b1a8",
   "metadata": {},
   "source": [
    "Try to find the solution, but if you really cannot find it, use these spoilers.\n",
    "\n",
    "Forward\n",
    "<div class=\"spoiler\">\n",
    "return self.b + np.sum(self.w * x[:,None,:], axis=-1)\n",
    "</div>\n",
    "\n",
    "Backward\n",
    "<div class=\"spoiler\">\n",
    "self.grad_w += np.sum(self.input[:, None, :] * dObj_dOut[:, :, None], axis=0)\n",
    "<br/>\n",
    "self.grad_b += np.sum(dObj_dOut, axis=0)\n",
    "<br/>\n",
    "return np.sum(self.w * dObj_dOut[:, :, None], axis=1)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea782b-c104-4928-982e-c03fc86cce67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89c791-fec4-4fa3-a251-f3b756d45b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed or as an example, a ReLU activation\n",
    "\n",
    "class ReLU(Function):\n",
    "    def compute_forward(self, x):\n",
    "        # we store input positivity to be able to do the backward pass\n",
    "        self.input_is_positive = x>0\n",
    "        is_neg = np.invert(self.input_is_positive)\n",
    "        out = np.copy(x) # we could also do it in place as far as ReLU is concerned\n",
    "        out[is_neg] = 0\n",
    "        return out\n",
    "    def compute_backward(self, dObj_dOut):\n",
    "        # for negative inputs, the output was 0*x so 0 so it is constant so the derivative is 0\n",
    "        # for positive inputs, the outpput was x so the derivative is 1\n",
    "        # the derivative of the objective with respect to the input is the derivative of ReLU × the derivative of the objective with respect to the output\n",
    "        dObj_dIn = self.input_is_positive * dObj_dOut\n",
    "        return dObj_dIn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314da4f6-2d77-4a74-a80c-2ed27396ae45",
   "metadata": {},
   "source": [
    "### A small dataset and a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8021a-cfc9-43f6-a397-455e2ff1b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_pred(model, r=5, show=True):\n",
    "    support = np.linspace(-r, r, 51)\n",
    "    rx, ry = np.meshgrid(support, support)\n",
    "    pred = model(np.stack([rx.flatten(), ry.flatten()]).T)\n",
    "    #plt.imshow(pred.reshape(rx.shape), extent=[rx.min(), rx.max(), ry.min(), ry.max()])\n",
    "    plt.contourf(rx, ry, pred.reshape(rx.shape), vmin=0, vmax=1, levels=np.linspace(0, 1, 21), alpha=0.25)\n",
    "    plt.colorbar()\n",
    "    if show: plt.show()\n",
    "\n",
    "# just a few 2d points, with binary labels, the model produces a probaility of 1\n",
    "simple_X = np.array([[1, 2], [2, 1], [3, 2], [2, 3]])\n",
    "simple_Y = np.array([1, 1, 0, 0])[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0be569-fe95-48cb-94e4-11aae019ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an perceptron (two dimensional input, so 2, and single output so 1)\n",
    "m = Sequential([Linear(2, 1), Sigmoid()])\n",
    "\n",
    "# Training loop\n",
    "loss = BinaryCrossEntropy()\n",
    "STEP = 100 # display every STEP steps\n",
    "for epoch in range(10*STEP+1):\n",
    "    for gp in m.get_params_and_grads()[1]:\n",
    "        gp.fill(0)\n",
    "    loss.target = simple_Y\n",
    "    pred = m(simple_X)\n",
    "    error = loss(pred, simple_Y)\n",
    "    dError_dPred = loss.compute_backward()\n",
    "    m.compute_backward(dError_dPred)\n",
    "    for p, gp in zip(*m.get_params_and_grads()):\n",
    "        p -= 0.1 * gp\n",
    "    if epoch%STEP == 0:\n",
    "        print(np.sum(error), error.flatten(), pred.flatten())#, m.get_params_and_grads()[0])\n",
    "        show_pred(m, show=False)\n",
    "        plt.scatter(simple_X[:,0], simple_X[:,1], c=simple_Y)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02af143-ea3d-440b-b0dc-9478f8c2d920",
   "metadata": {},
   "source": [
    "### Going further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7250c-ffa4-47a7-8f2e-30157b49dd51",
   "metadata": {},
   "source": [
    "**TODO for you**\n",
    "- try an few MLPs,\n",
    "- try a dataset with more points and a more complicated boundary,\n",
    "- compare different architectures (shallow, deep, minimal, over-parametrized, ...),\n",
    "- propose and use new Function classes\n",
    "- extract the concept of Optimizer that is instantiated with the model (and possibly optimizer parameters), and that is called at the beginning of the loop with `opt.zero_grad()` and after each backward with `opt.step()` \n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341c5c0-5eb6-4bab-936b-7321f7cfa381",
   "metadata": {},
   "source": [
    "an MLP\n",
    "\n",
    "<div class=\"spoiler\">\n",
    "    m = Sequential([Linear(2, 10), ReLU(), Linear(10, 10), ReLU(), Linear(10, 1), Sigmoid()])\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671061fc-9db5-4bd6-99fb-eef17b095133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
